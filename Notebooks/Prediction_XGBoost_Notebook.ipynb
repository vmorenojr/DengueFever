{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting with XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime as dt\n",
    "from datetime import timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "np.random.seed(12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados = pd.read_csv('../Dados/dengue_por_habitante.csv.gz')\n",
    "\n",
    "datasets = {}\n",
    "municipios = dados['municipio'].unique()\n",
    "\n",
    "for city in municipios:\n",
    "    df = dados[(dados['dt_sintoma'] >= '2006-01-01') & (dados['municipio'] == city)].copy()\n",
    "    df.drop(['co_municipio', 'municipio', 'estado', 'UF', 'regiao', 'latitude', 'longitude',\n",
    "            'populacao'], axis=1, inplace=True)\n",
    "    df.dt_sintoma = pd.to_datetime(df.dt_sintoma, format = '%Y-%m-%d %H:%M:%S')\n",
    "    datasets[city] = df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split time series\n",
    "def split_data(df, dates, split_date):\n",
    "    return df[df[dates] <= split_date].copy(), \\\n",
    "           df[df[dates] >  split_date].copy()\n",
    "\n",
    "# Create and append time series features\n",
    "def create_features(df, dates):\n",
    "    df_new = df.copy()\n",
    "    df_new['year'] = df[dates].dt.year\n",
    "    df_new['quarter'] = df[dates].dt.quarter\n",
    "    df_new['month'] = df[dates].dt.month\n",
    "    df_new['weekofyear'] = df[dates].dt.weekofyear\n",
    "    df_new['dayofyear'] = df[dates].dt.dayofyear\n",
    "    df_new['dayofmonth'] = df[dates].dt.day\n",
    "    df_new.drop([dates, 'ocorrencias', 'por_habitante'], \n",
    "                axis=1, inplace=True)\n",
    "    return df_new\n",
    "\n",
    "# Create lagged outcome\n",
    "def create_lagged(df, dates, outcome, first_date, last_date, timedelta_lag):\n",
    "    first_lag_date = first_date - timedelta_lag\n",
    "    last_lag_date = last_date - timedelta_lag\n",
    "    lagged = df[(df[dates] >= first_lag_date) &\n",
    "                (df[dates] <= last_lag_date)][outcome]\n",
    "    return lagged\n",
    "\n",
    "# Plot train and test time series\n",
    "def plot_ts(df_train, df_test, dates, outcome):#, plt_name):\n",
    "    fig=plt.figure(figsize=(10,5))\n",
    "    plt.title('Training and Test Time Series')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Number of reports')\n",
    "    plt.plot(df_train[dates], df_train[outcome], label='Training data')\n",
    "    plt.plot(df_test[dates], df_test[outcome], label='Test data')\n",
    "    plt.legend()\n",
    "    return fig\n",
    "    \n",
    "# Forecast on test set\n",
    "def plot_performance(base_df, test_df, predicted_df, dates, predicted, \n",
    "                     date_from, date_to, title=None):\n",
    "    fig = plt.figure(figsize=(10,5))\n",
    "    if title == None:\n",
    "        plt.title('From {0} to {1}'.format(date_from, date_to))\n",
    "    else:\n",
    "        plt.title(title)\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Number of reports')\n",
    "    plt.plot(base_df[dates], base_df[predicted], label='Data')\n",
    "    plt.plot(test_df[dates], predicted_df, label='Prediction')\n",
    "    plt.legend()\n",
    "    plt.xlim(left=date_from, right=date_to)\n",
    "    return fig\n",
    "\n",
    "# Calculates MAPE given y_true and y_pred\n",
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "# Generate datasets, run XGBoost and present results\n",
    "def analysis(datasets, municipios, target_city, dates, outcome, \n",
    "             start_date, end_date, split_date, \n",
    "             plt_name_base, pars, lag_weeks=0, range_weeks=8,\n",
    "             lagged=True, date_features=True, show=False):\n",
    "    \n",
    "    df_city = datasets[city].copy()\n",
    "    df_city.set_index(dates, drop=False, inplace=True)\n",
    "\n",
    "    if lagged:\n",
    "        # Create lagged outcome and add as new columns\n",
    "        for capital in municipios:\n",
    "            df_capital = datasets[capital].copy()\n",
    "            df_capital.set_index(dates, drop=False, inplace=True)\n",
    "            for lag in range(1, range_weeks+1):\n",
    "                name = capital +'_' + str(lag)\n",
    "                lagged_ocorrencias = create_lagged(df=df_capital, \n",
    "                                                   dates=dates, \n",
    "                                                   outcome=outcome, \n",
    "                                                   first_date=start_date-timedelta(weeks=lag_weeks-1),\n",
    "                                                   last_date=end_date-timedelta(weeks=lag_weeks-1), \n",
    "                                                   timedelta_lag=timedelta(weeks=lag))\n",
    "                lagged_ocorrencias.index += timedelta(weeks=lag_weeks-1) + timedelta(weeks=lag)\n",
    "                df_city[name] = lagged_ocorrencias\n",
    "                \n",
    "    df_city = df_city[(df_city[dates]>= start_date) & \n",
    "            (df_city[dates]<= end_date)]\n",
    "\n",
    "    # Split the time series \n",
    "    train, test = split_data(df_city, dates, split_date)\n",
    "\n",
    "    # Plot time series\n",
    "    plot_ts(df_train=train, \n",
    "            df_test=test, \n",
    "            dates=dates, \n",
    "            outcome=outcome)\n",
    "    plt.savefig('../Plots/ts-' + plt_name_base + '.png')\n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()\n",
    "    \n",
    "    # Create time series features\n",
    "    if date_features:\n",
    "        X_train, y_train = create_features(train, dates), train[outcome]\n",
    "        X_test, y_test = create_features(test, dates), test[outcome]\n",
    "    else:\n",
    "        X_train, y_train = train.drop([dates,'ocorrencias', 'por_habitante'], axis=1), train[outcome]\n",
    "        X_test, y_test = test.drop([dates,'ocorrencias', 'por_habitante'], axis=1), test[outcome]\n",
    "\n",
    "        # Create and Train XGBoost Model\n",
    "    \n",
    "    # Fit the model\n",
    "    reg = xgb.XGBRegressor(params=pars, \n",
    "                           objective='reg:squarederror',\n",
    "                           seed=123)\n",
    "    reg.fit(X_train, y_train,\n",
    "            eval_set=[(X_train, y_train), (X_test, y_test)],\n",
    "            early_stopping_rounds=50, \n",
    "            verbose=False)\n",
    "\n",
    "    # Plot and save feature importances\n",
    "    plt.rcParams['figure.figsize'] = (15, 10)\n",
    "    xgb.plot_importance(reg, height=.9, max_num_features=30)\n",
    "    plt.savefig('../Plots/imp-' + plt_name_base + '.png')\n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()\n",
    "    \n",
    "    reg_importance = pd.DataFrame(reg.feature_importances_, columns=['Importance'])*1000\n",
    "    reg_importance['Features'] = X_train.columns\n",
    "    city_lag = reg_importance.Features.str.split('_', expand=True)\n",
    "    reg_importance = reg_importance.merge(city_lag, left_index=True, right_index=True)\\\n",
    "                                   .drop('Features', axis=1)\n",
    "    reg_importance.columns = ['Importance', 'City', 'Lag']\n",
    "    reg_importance.to_csv('XGB_fit/importances.csv', index=False)\n",
    "    # Generate forecast\n",
    "    y_pred = reg.predict(X_test)\n",
    "\n",
    "    # Plot forecast\n",
    "    plot_performance(base_df=df_city, \n",
    "                     test_df=test,\n",
    "                     predicted_df=y_pred,\n",
    "                     dates=dates, \n",
    "                     predicted=outcome, \n",
    "                     date_from=start_date,\n",
    "                     date_to=end_date, \n",
    "                     title='Original and Predicted Data')\n",
    "    #plt.savefig('Plots/tspred-' + plt_name_base + '.png')\n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()\n",
    "    \n",
    "    plot_performance(base_df=test, \n",
    "                     test_df=test,\n",
    "                     predicted_df=y_pred,\n",
    "                     dates=dates, \n",
    "                     predicted=outcome, \n",
    "                     date_from=split_date,\n",
    "                     date_to=end_date, \n",
    "                     title='Test and Predicted Data')\n",
    "    plt.savefig('../Plots/pred-' + plt_name_base + '.png')\n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()\n",
    "    \n",
    "    RMSE = np.sqrt(mean_squared_error(y_true=y_test, y_pred=y_pred))\n",
    "    MAE = mean_absolute_error(y_true=y_test, y_pred=y_pred)\n",
    "    MAPE = mean_absolute_percentage_error(y_true=y_test, y_pred=y_pred)\n",
    "    return RMSE, MAE, MAPE\n",
    "\n",
    "# Function for validation and test of model\n",
    "def training(datasets, municipios, target_city, dates, outcome, \n",
    "             start_date, end_date, split_date,\n",
    "             n_estimators, learning_rate, min_child_weight,\n",
    "             max_depth, gamma, subsample, colsample_bytree,\n",
    "             lag_weeks=0, range_weeks=8):\n",
    "    \n",
    "    df_city = datasets[city].copy()\n",
    "    df_city.set_index(dates, drop=False, inplace=True)\n",
    "\n",
    "    for capital in municipios:\n",
    "        df_capital = datasets[capital].copy()\n",
    "        df_capital.set_index(dates, drop=False, inplace=True)\n",
    "        for lag in range(1, range_weeks+1):\n",
    "            name = capital +'_' + str(lag)\n",
    "            lagged_ocorrencias = create_lagged(df=df_capital, \n",
    "                                               dates=dates, \n",
    "                                               outcome=outcome, \n",
    "                                               first_date=start_date-timedelta(weeks=lag_weeks-1),\n",
    "                                               last_date=end_date-timedelta(weeks=lag_weeks-1), \n",
    "                                               timedelta_lag=timedelta(weeks=lag))\n",
    "            lagged_ocorrencias.index += timedelta(weeks=lag_weeks-1) + timedelta(weeks=lag)\n",
    "            df_city[name] = lagged_ocorrencias\n",
    "                \n",
    "    df_city = df_city[(df_city[dates]>= start_date) & \n",
    "            (df_city[dates]<= end_date)]\n",
    "\n",
    "    # Split the time series\n",
    "    train, test = split_data(df_city, dates, split_date)\n",
    "\n",
    "    X_train, y_train = train.drop([dates,'ocorrencias', 'por_habitante'], axis=1), train[outcome]\n",
    "    X_test, y_test = test.drop([dates,'ocorrencias', 'por_habitante'], axis=1), test[outcome]\n",
    "\n",
    "    # Create and Train XGBoost Model\n",
    "    reg = xgb.XGBRegressor(n_estimators=n_estimators, learning_rate=learning_rate,\n",
    "                           min_child_weight=min_child_weight, max_depth=max_depth, \n",
    "                           gamma=gamma, subsample=subsample, colsample_bytree=colsample_bytree,\n",
    "                           objective='reg:squarederror', seed=123)\n",
    "    reg.fit(X_train, y_train,\n",
    "            eval_set=[(X_train, y_train), (X_test, y_test)],\n",
    "            early_stopping_rounds=50, \n",
    "            verbose=False)\n",
    "\n",
    "    # Generate forecast\n",
    "    y_pred = reg.predict(X_test)\n",
    "\n",
    "    return np.sqrt(mean_squared_error(y_true=y_test, y_pred=y_pred))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length mismatch: Expected axis has 2 elements, new values have 3 elements",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-08e4b42f310f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m                           \u001b[0mpars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'n_estimators'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                           \u001b[0mdate_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                           show = False)\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'RMSE'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRMSE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'MAE'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mMAE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'MAPE'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mMAPE\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../XGB_fit/baseline.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-d8b46b2bbba7>\u001b[0m in \u001b[0;36manalysis\u001b[0;34m(datasets, municipios, target_city, dates, outcome, start_date, end_date, split_date, plt_name_base, pars, lag_weeks, range_weeks, lagged, date_features, show)\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0mreg_importance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreg_importance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcity_lag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                                    \u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Features'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     \u001b[0mreg_importance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Importance'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'City'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Lag'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m     \u001b[0mreg_importance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'XGB_fit/importances.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;31m# Generate forecast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   5190\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5191\u001b[0m             \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5192\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5193\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5194\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/properties.pyx\u001b[0m in \u001b[0;36mpandas._libs.properties.AxisProperty.__set__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_set_axis\u001b[0;34m(self, axis, labels)\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 690\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    691\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clear_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mset_axis\u001b[0;34m(self, axis, new_labels)\u001b[0m\n\u001b[1;32m    181\u001b[0m             raise ValueError(\n\u001b[1;32m    182\u001b[0m                 \u001b[0;34m\"Length mismatch: Expected axis has {old} elements, new \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m                 \u001b[0;34m\"values have {new} elements\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mold_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m             )\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Length mismatch: Expected axis has 2 elements, new values have 3 elements"
     ]
    }
   ],
   "source": [
    "# Define time series start and end, and split date\n",
    "end_date = dt.strptime('2019-05-05', '%Y-%m-%d')\n",
    "start_date = dt.strptime('2010-05-05', '%Y-%m-%d')\n",
    "split_date = dt.strptime('2018-05-05', '%Y-%m-%d')\n",
    "\n",
    "# Define target city\n",
    "city = 'Belo Horizonte'\n",
    "\n",
    "# Define outcome and date features\n",
    "outcome = 'ocorrencias'\n",
    "dates = 'dt_sintoma'\n",
    "\n",
    "# --------\n",
    "# Baseline\n",
    "# --------\n",
    "\n",
    "# Run analysis and store fit results\n",
    "RMSE, MAE, MAPE = analysis(datasets=datasets, municipios=['Belo Horizonte'], target_city=city, \n",
    "                          dates=dates, outcome=outcome,\n",
    "                          start_date=start_date, end_date=end_date, split_date=split_date, \n",
    "                          plt_name_base='base', \n",
    "                          lagged=False,\n",
    "                          pars={'n_estimators': 1000},\n",
    "                          date_features=True,\n",
    "                          show = False)\n",
    "results = pd.DataFrame({'RMSE': RMSE, 'MAE': MAE, 'MAPE': MAPE}, index=[0])\n",
    "results.to_csv('../XGB_fit/baseline.csv', index=False)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline with lags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe to store results\n",
    "results = pd.DataFrame(columns=['All cities', 'Lag (weeks)', 'Range (weeks)', 'RMSE', 'MAE', 'MAPE'])\n",
    "\n",
    "# Run analysis and store fit results\n",
    "for i in [4, 12, 26, 52]:\n",
    "    for j in [8, 12, 26, 52]:\n",
    "        plt_base = 'base-l' + str(i) + 'r' + str(j)# + 'caps'\n",
    "        RMSE, MAE, MAPE = analysis(datasets=datasets, municipios=['Belo Horizonte'], target_city=city, \n",
    "                                  dates=dates, outcome=outcome,\n",
    "                                  start_date=start_date, end_date=end_date, split_date=split_date, \n",
    "                                  plt_name_base=plt_base, \n",
    "                                  pars={'n_estimators': 1000},\n",
    "                                  lag_weeks=i,\n",
    "                                  range_weeks=j,\n",
    "                                  lagged=True,\n",
    "                                  date_features=True,\n",
    "                                  show = False)\n",
    "        results = results.append({'All cities': 'No', \n",
    "                                'Lag (weeks)': i, \n",
    "                                'Range (weeks)': j, \n",
    "                                'RMSE': RMSE, 'MAE': MAE, 'MAPE': MAPE}, ignore_index=True)\n",
    "results.to_csv('XGB_fit/baseline-lags.csv', index=False)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model with all capitals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe to store results\n",
    "results = pd.DataFrame(columns=['All cities', 'Lag (weeks)', 'Range (weeks)', 'RMSE', 'MAE', 'MAPE'])\n",
    "\n",
    "# Run analysis and store fit results\n",
    "for i in [4, 12, 26, 52]:\n",
    "    for j in [8, 12, 26, 52]:\n",
    "        plt_base = 'all-l' + str(i) + 'r' + str(j)# + 'caps'\n",
    "        RMSE, MAE, MAPE = analysis(datasets=datasets, municipios=municipios, target_city=city, \n",
    "                                  dates=dates, outcome=outcome,\n",
    "                                  start_date=start_date, end_date=end_date, split_date=split_date, \n",
    "                                  plt_name_base=plt_base, \n",
    "                                  pars={'n_estimators': 1000},\n",
    "                                  lag_weeks=i,\n",
    "                                  range_weeks=j,\n",
    "                                  lagged=True,\n",
    "                                  date_features=True,\n",
    "                                  show = False)\n",
    "        results = results.append({'All cities': 'Yes', \n",
    "                                'Lag (weeks)': i, \n",
    "                                'Range (weeks)': j, \n",
    "                                'RMSE': RMSE, 'MAE': MAE, 'MAPE': MAPE}, ignore_index=True)\n",
    "results.to_csv('XGB_fit/allcaps.csv', index=False)\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model with all capitals and no time-related features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe to store results\n",
    "results = pd.DataFrame(columns=['All cities', 'Lag (weeks)', 'Range (weeks)', 'RMSE', 'MAE', 'MAPE'])\n",
    "\n",
    "# Run analysis and store fit results\n",
    "for i in [4]:\n",
    "    for j in [8, 12, 26, 52]:\n",
    "        plt_base = 'all-no-time-l' + str(i) + 'r' + str(j)# + 'caps'\n",
    "        RMSE, MAE, MAPE = analysis(datasets=datasets, municipios=municipios, target_city=city, \n",
    "                                  dates=dates, outcome=outcome,\n",
    "                                  start_date=start_date, end_date=end_date, split_date=split_date, \n",
    "                                  plt_name_base=plt_base, \n",
    "                                  pars={'n_estimators': 1000},\n",
    "                                  lag_weeks=i,\n",
    "                                  range_weeks=j,\n",
    "                                  lagged=True,\n",
    "                                  date_features=False,\n",
    "                                  show = False)\n",
    "        results = results.append({'All cities': 'Yes', \n",
    "                                'Lag (weeks)': i, \n",
    "                                'Range (weeks)': j, \n",
    "                                'RMSE': RMSE, 'MAE': MAE, 'MAPE': MAPE}, ignore_index=True)\n",
    "results.to_csv('XGB_fit/allcapsnotime.csv', index=False)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning of hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = [500]\n",
    "learning_rate = [.275]\n",
    "min_child_weight = [5]\n",
    "max_depth = [5]\n",
    "gamma = [0]\n",
    "subsample = [1]\n",
    "colsample_bytree=[1]\n",
    "\n",
    "results = pd.DataFrame(columns=['n_estimators', 'learning_rate', 'min_child_weight', \n",
    "                                'max_depth', 'gamma', 'subsample', 'colsample_bytree', \n",
    "                                'RMSE'])\n",
    "\n",
    "for est in n_estimators:\n",
    "    for learn in learning_rate:\n",
    "        for min_child in min_child_weight:\n",
    "            for max_dpth in max_depth:\n",
    "                for gma in gamma:\n",
    "                    for sub in subsample:\n",
    "                        for col in colsample_bytree:\n",
    "                            RMSE = training(datasets=datasets, municipios=municipios, target_city=city,\n",
    "                                            dates=dates, outcome=outcome,\n",
    "                                            start_date=start_date, \n",
    "                                            end_date=(end_date - timedelta(weeks=12)), \n",
    "                                            split_date=(end_date - timedelta(weeks=24)), \n",
    "                                            lag_weeks=4, range_weeks=26,\n",
    "                                            n_estimators=est,\n",
    "                                            learning_rate=learn,\n",
    "                                            min_child_weight=min_child,\n",
    "                                            max_depth=max_dpth, \n",
    "                                            gamma=gma, \n",
    "                                            subsample=sub,\n",
    "                                            colsample_bytree=col)\n",
    "                                            \n",
    "                            results = results.append({'n_estimators': est, 'learning_rate': learn,\n",
    "                                                    'min_child_weight': min_child, 'max_depth': max_dpth, \n",
    "                                                    'gamma':gma, 'subsample': sub, 'colsample_bytree': col,\n",
    "                                                    'RMSE': RMSE}, \n",
    "                                                    ignore_index=True)\n",
    "                            \n",
    "results.to_csv('XGB_fit/tuning-learning.csv', index=False)\n",
    "print(results[results.RMSE == min(results.RMSE)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "end = start_date + timedelta(weeks = 36)\n",
    "\n",
    "while end <= end_date:\n",
    "    results.append(training(datasets=datasets, municipios=municipios, target_city=city,\n",
    "                            dates=dates, outcome=outcome,\n",
    "                            start_date=start_date, \n",
    "                            end_date=end, \n",
    "                            split_date=(end - timedelta(weeks=12)), \n",
    "                            lag_weeks=4, range_weeks=26,\n",
    "                            n_estimators=500,\n",
    "                            learning_rate=.275,\n",
    "                            min_child_weight=5,\n",
    "                            max_depth=5, \n",
    "                            gamma=0, \n",
    "                            subsample=1,\n",
    "                            colsample_bytree=1))\n",
    "    end += timedelta(weeks=12)\n",
    "\n",
    "print('Mean RMSE = ', np.mean(results))\n",
    "\n",
    "# Predicting the outcome for the last year\n",
    "RMSE, MAE, MAPE = analysis(datasets=datasets, municipios=municipios, target_city=city, \n",
    "                           dates=dates, outcome=outcome,\n",
    "                           start_date=start_date, end_date=end_date, split_date=split_date, \n",
    "                           plt_name_base='final-no-time-l4r26', \n",
    "                           pars={'n_estimators': 500,\n",
    "                                 'learning_rate': .275,\n",
    "                                 'min_child_weight': 5,\n",
    "                                 'max_depth': 5,\n",
    "                                 'gamma': 0,\n",
    "                                 'subsample': 1,\n",
    "                                 'colsample_bytree': 1},\n",
    "                           lag_weeks=4, range_weeks=26,\n",
    "                           lagged=True, date_features=False, show = False)\n",
    "\n",
    "print('RMSE: ', RMSE, 'MAE: ', MAE, 'MAPE: ', MAPE)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
